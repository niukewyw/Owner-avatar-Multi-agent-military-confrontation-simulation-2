# 多智能体强化学习对抗训练推荐流程

## 核心方法：迭代自我对弈（Iterative Self-Play）

敌我双方使用相同的强化学习算法（推荐以 MAPPO 为主算法，后续可融合 QMIX/VDN），通过交替训练实现策略的动态共演化，最终形成持续的“军备竞赛”机制，获得更强、更鲁棒的对抗策略。

## 推荐训练流程

### 1. 初始化阶段
- 双方（我方与敌方）策略均从规则控制器初始化：
  - 侦察类智能体使用 `ReconController`。
  - 其他类型智能体使用 `AggressiveController`。
- 可选预训练：
  - 使用行为克隆（Behavior Cloning）将规则控制器的行为数据蒸馏到初始神经网络策略中，提升训练起点和早期稳定性。

### 2. 迭代训练主循环
重复执行以下步骤 A → B 多轮（建议 20~100 轮，视计算资源而定）：

- **步骤 A：训练敌方策略**
  - 固定当前最优的我方模型作为对手。
  - 使用 MAPPO 算法收集 rollout 数据（多 episode 并行采样）。
  - 更新敌方策略若干步数，或直至敌方对当前我方模型的胜率显著提升（例如 >65%~70%）。

- **步骤 B：训练我方策略**
  - 固定最新的敌方模型作为对手。
  - 使用 MAPPO 算法收集 rollout 数据。
  - 更新我方策略若干步数，或直至我方对当前敌方模型的胜率显著提升（例如 >65%~70%）。

### 3. 每轮评估与监控
- 每完成一轮（A+B）后，使用全新随机种子进行独立评估（不用于训练的 episode）。
- 记录关键指标：
  - 我方胜率 / 敌方胜率 / 平局率
  - 平均 episode 奖励（双方各自）
  - 平均 episode 长度
  - 其他行为指标（如总伤害输出、存活率、侦察覆盖率）
- 理想趋势：胜率逐渐趋近 50% 左右，表示双方达到相对均衡的纳什均衡状态。

### 4. 进阶优化：引入种群自我对弈（Population-Based Self-Play）
为提升策略多样性和鲁棒性，可在基础迭代自我对弈上增加对手历史版本：
- 维护一个对手模型池（opponent pool），每隔若干轮保存当前策略检查点（建议保留 5~20 个历史版本）。
- 在训练当前策略时，从对手池中随机采样对手：
  - 示例采样比例：70% 最新版本 + 30% 历史版本。
- 优势：有效防止策略遗忘（catastrophic forgetting）和循环震荡，提升策略在不同对手下的泛化能力。

### 5. 算法融合建议
- **主算法**：MAPPO（处理异构智能体、部分可观测、动作执行）。
- **价值分解扩展**：在同方内部引入 QMIX 或 VDN，实现集中式训练分布式执行（CTDE），提升团队协作（如侦察信息共享、火炮协同打击）。
- **最终推荐架构**：MAPPO 产生动作概率 + QMIX/VDN 进行全局价值评估。

## 训练停止条件
- 达到预设总训练步数或 episode 数。
- 胜率在多轮评估中稳定波动于 45%~55% 区间。
- 奖励曲线趋于平稳且无明显退化。

此流程能够产生真正具备对抗能力的智能体策略，适用于后续在不同地图（如 attack_focused、defense_focused 等）上的迁移与评估。